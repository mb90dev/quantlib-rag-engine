# QuantLib RAG Assistant (Internal Documentation Search)

This project implements a **Retrieval-Augmented Generation (RAG)** system over  
the official **QuantLib-Python documentation**, downloaded and processed locally.

It is designed as a **portfolio-ready example of building an enterprise-style RAG**:
- local documentation â†’ markdown
- text chunking with markdown-aware splitter
- embeddings with **BAAI/bge-m3**
- vector database using **ChromaDB**
- **local LLM (Mistral via Ollama)** used only as a *document-bound reasoning engine*
- Streamlit frontend with two modes:
  - **Search only (retriever)**
  - **Docs-based answer (LLM constrained to documentation)**

The model is **not allowed to use outside knowledge**.  
All answers must come from retrieved QuantLib documentation.

---

## ğŸ— Project Structure

```
quantlib-rag-engine/
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ quantlib_rag/
â”‚       â”œâ”€â”€ ingestion/
â”‚       â”‚   â”œâ”€â”€ download_quantlib_docs.py
â”‚       â”‚   â””â”€â”€ build_index.py
â”‚       â”œâ”€â”€ rag/
â”‚       â”‚   â”œâ”€â”€ quantlib_index.py
â”‚       â”‚   â”œâ”€â”€ quantlib_quote_assistant.py
â”‚       â”‚   â””â”€â”€ ui_streamlit.py
â”‚       â””â”€â”€ config.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ quantlib_md/     â† generated automatically
â”‚
â”œâ”€â”€ db/
â”‚   â””â”€â”€ quantlib_chroma_bge_md/   â† generated automatically
â”‚
â”œâ”€â”€ main.py      â† unified runner (download â†’ index â†’ UI)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Install dependencies
```bash
pip install -r requirements.txt
```

### 2. Install Ollama (for local LLM)
https://ollama.ai/install

### 3. Pull the Mistral model
```bash
ollama pull mistral
```

### 4. Run the automatic bootstrap (docs â†’ index â†’ UI)

```bash
python main.py
```

This will:

1. Download documentation from ReadTheDocs  
2. Convert it to `.md` files  
3. Build Chroma vector index (`db/quantlib_chroma_bge_md/`)  
4. Launch the Streamlit UI at  

```
http://localhost:8501
```

---

## ğŸ§  System Overview

### Retrieval
- markdown chunking via `MarkdownHeaderTextSplitter`
- embeddings: **BAAI/bge-m3**
- vector DB: **ChromaDB** (persistent)

### Reasoning
- local LLM via **Ollama**
- constrained mode:
  - LLM may **only use retrieved documentation**
  - zero external knowledge
  - ideal for *internal-company documentation RAGs*

---

## ğŸ–¥ UI Modes

### 1. ğŸ” Search only (retriever)
Shows raw documentation chunks retrieved by vector search.

### 2. ğŸ§¾ Docs-based answer (LLM)
LLM receives:
- user question
- retrieved chunks
- strict system instructions:
  - *answer only using provided context*
  - *do not invent API*
  - *never use outside knowledge*

---

## ğŸ§ª Testing the RAG

Example queries:

- â€œGive a QuantLib-Python example of building a flat yield curve using `FlatForward`.â€
- â€œHow to compute year fraction using Actual/360?â€
- â€œWhat are day count conventions supported by QuantLib-Python?â€
- â€œShow the parameters of `Schedule` constructor.â€

---

## ğŸ“¦ Deployment Notes

This project is standalone:
- no external APIs needed
- works offline once downloaded the first time
- ideal for demonstrating RAG engineering skills

---

## ğŸ™Œ Credits

Built as a personal research project to demonstrate practical RAG system design  
for internal enterprise documentation.

```
Author: mb90dev
```
