# src/quantlib_rag/graph/guarded_rag.py

from dataclasses import dataclass
from typing import Dict, Any, Optional, List

from langgraph.graph import StateGraph
from langchain_core.documents import Document
from langchain_core.runnables import Runnable

from .state import GuardedRAGState
from ..rag.quantlib_assistant import QuantLibQuoteAssistant
from ..rag.llm_judge import verify_answer


@dataclass
class GuardedQuantLibRAG:
    """
    Guarded RAG pipeline for QuantLib-Python documentation.

    Flow:
      1) generate_answer: call QuantLibQuoteAssistant.quote_only_answer(...)
      2) retrieve_context: re-run retrieval via QuantLibIndex for verification
      3) verify_answer: use Groq judge (LLM-as-judge)
      4) finalize_*: either keep answer or replace it with safe 'I don't know...'
    """

    assistant: Any
    judge_llm: Runnable
    k_for_verification: Optional[int] = None  # if None -> assistant.k_default

    # --------- INTERNAL HELPERS --------- #

    def _get_verification_docs(self, question_en: str, k: Optional[int]) -> List[Document]:
        """
        Uses the underlying QuantLibIndex retriever to fetch docs
        for verification purposes.
        """
        if k is None:
            k = self.assistant.k_default

        retriever = self.assistant.index.get_retriever(k=k)
        docs = retriever.invoke(question_en)
        return docs

    # --------- NODES --------- #

    def _generate_node(self, state: GuardedRAGState) -> GuardedRAGState:
        """
        Call the existing QuantLibQuoteAssistant.quote_only_answer(...) to:
        - reuse JSON + semantic cache
        - reuse quote-only system prompt
        """
        result = self.assistant.quote_only_answer(
            question_en=state["question_en"],
            k=self.assistant.k_default,
        )

        # spodziewany format:
        # {
        #   "question_en": ...,
        #   "answer_en": ...,
        #   "sources": [...]
        # }
        state["answer_en"] = result.get("answer_en")
        state["sources"] = result.get("sources", [])
        return state

    def _retrieve_context_node(self, state: GuardedRAGState) -> GuardedRAGState:
        """
        Retrieve full Document chunks for verification via judge LLM.
        Uses the same QuantLibIndex as the assistant.
        """
        k_verify = self.k_for_verification or self.assistant.k_default
        docs = self._get_verification_docs(state["question_en"], k=k_verify)
        state["contexts"] = docs
        return state

    def _verify_node(self, state: GuardedRAGState) -> GuardedRAGState:
        """
        Run LLM-as-judge on (question, answer, contexts).
        """
        if not state["answer_en"]:
            # brak odpowiedzi -> traktujemy jak out_of_scope / not_grounded
            state["verification"] = {
                "is_grounded": False,
                "out_of_scope": True,
                "faithfulness_score": 1,
                "reason": "No answer generated by the assistant.",
            }
            return state

        if not state["contexts"]:
            # brak kontekstu -> też traktujemy jako nieugruntowane
            state["verification"] = {
                "is_grounded": False,
                "out_of_scope": True,
                "faithfulness_score": 1,
                "reason": "No context retrieved for verification.",
            }
            return state

        state["verification"] = verify_answer(
            llm=self.judge_llm,
            question=state["question_en"],
            answer=state["answer_en"],
            contexts=state["contexts"],
        )
        return state

    @staticmethod
    def _verification_router(state: GuardedRAGState) -> str:
        """
        Decide which finalize node to go to based on verification result.
        """
        v = state.get("verification") or {}
        is_grounded = v.get("is_grounded", False)
        out_of_scope = v.get("out_of_scope", False)

        if is_grounded and not out_of_scope:
            return "ok"
        else:
            return "reject"

    @staticmethod
    def _finalize_ok(state: GuardedRAGState) -> GuardedRAGState:
        # answer_en zostaje taka, jak wygenerował QuantLibQuoteAssistant
        return state

    @staticmethod
    def _finalize_reject(state: GuardedRAGState) -> GuardedRAGState:
        """
        If the judge says the answer is not grounded or out-of-scope,
        we replace it with a safe message.
        """
        v = state.get("verification") or {}
        out_of_scope = v.get("out_of_scope", False)

        if out_of_scope:
            state["answer_en"] = (
                "I don't know based on the provided documentation. "
                "The question also seems to go beyond the internal QuantLib-Python docs."
            )
        else:
            state["answer_en"] = "I don't know based on the provided documentation."

        return state

    # --------- GRAPH BUILD & PUBLIC API --------- #

    def build(self):
        """
        Build and compile the LangGraph graph.
        """
        builder = StateGraph(GuardedRAGState)

        builder.add_node("generate_answer", self._generate_node)
        builder.add_node("retrieve_context", self._retrieve_context_node)
        builder.add_node("verify_answer", self._verify_node)
        builder.add_node("finalize_ok", self._finalize_ok)
        builder.add_node("finalize_reject", self._finalize_reject)

        builder.set_entry_point("generate_answer")
        builder.add_edge("generate_answer", "retrieve_context")
        builder.add_edge("retrieve_context", "verify_answer")

        builder.add_conditional_edges(
            "verify_answer",
            self._verification_router,
            {
                "ok": "finalize_ok",
                "reject": "finalize_reject",
            },
        )

        return builder.compile()

    def invoke(self, question_en: str) -> Dict[str, Any]:
        """
        Convenience method to run a full guarded RAG flow for a single question.
        """
        graph = self.build()
        initial_state: GuardedRAGState = {
            "question_en": question_en,
            "answer_en": None,
            "sources": [],
            "contexts": [],
            "verification": None,
        }
        final_state = graph.invoke(initial_state)

        return {
            "question_en": question_en,
            "answer_en": final_state["answer_en"],
            "sources": final_state["sources"],
            "verification": final_state["verification"],
        }
